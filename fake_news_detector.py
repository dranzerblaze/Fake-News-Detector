# -*- coding: utf-8 -*-
"""Fake News Detector.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xapSjxrKbUEDlVCJR-NSJfSxqsu_Xupr
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

import pandas as pd
import numpy as np

link = "https://drive.google.com/open?id=1lDNHpcfjQ3uIm-0DaCkcteKyhjcLna_3"
fluff,id = link.split('=')
downloaded = drive.CreateFile({'id':id})
downloaded.GetContentFile('train.csv')
df = pd.read_csv('train.csv')
df.head()

MAX_SEQUENCE_LENGTH = 1000
MAX_NB_WORDS = 200000
EMBEDDING_DIM = 100
VALIDATION_SPLIT = 0.2

def clean_str(string):
    string = re.sub(r"\\", "", string)    
    string = re.sub(r"\'", "", string)    
    string = re.sub(r"\"", "", string)    
    return string.strip().lower()

df.columns

df.shape

import keras
from tensorflow.python.client import device_lib
from collections import defaultdict
import re
import sys
import os
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils.np_utils import to_categorical
from keras.layers import Embedding
from keras.layers import Dense, Input, Flatten
from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout
from keras.models import Model

texts = []
labels = []

for i in range(df.text.shape[0]):
    text1 = df.title[i]
    text2 = df.text[i]
    text = str(text1) +""+ str(text2)
    texts.append(text)
    labels.append(df.label[i])
    
tokenizer = Tokenizer(num_words=MAX_NB_WORDS)
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)

word_index = tokenizer.word_index

data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)
labels = to_categorical(np.asarray(labels),num_classes = 2)
print('Shape of data tensor:', data.shape)
print('Shape of label tensor:', labels.shape)

from sklearn.model_selection import train_test_split
indices = np.arange(data.shape[0])
np.random.shuffle(indices)
data = data[indices]
labels = labels[indices]
x_train, x_test, y_train, y_test = train_test_split( data, labels, test_size=0.20)
x_test, x_val, y_test, y_val = train_test_split( x_test, y_test, test_size=0.50)
print('Size of train, validation, test:', len(y_train), len(y_val), len(y_test))

# Commented out IPython magic to ensure Python compatibility.
from keras.models import Sequential
from keras.layers.convolutional import Conv3D
from keras.layers.convolutional_recurrent import ConvLSTM2D
from keras.layers.normalization import BatchNormalization
from matplotlib import pyplot as plt
from keras.layers import Dense, Embedding, LSTM, GRU

# %matplotlib inline

!wget http://nlp.stanford.edu/data/glove.6B.zip

!unzip glove*.zip

!pwd

!ls

GLOVE_DIR = "" 
embeddings_index = {}
f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding="utf8")
for line in f:
    values = line.split()
    #print(values[1:])
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()

print('Total %s word vectors in Glove.' % len(embeddings_index))

embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        # words not found in embedding index will be all-zeros.
        embedding_matrix[i] = embedding_vector
        
embedding_layer = Embedding(len(word_index) + 1,
                            EMBEDDING_DIM,
                            weights=[embedding_matrix],
                            input_length=MAX_SEQUENCE_LENGTH)

#Training LSTM Model
embedding_vecor_length = 32
lstm_model = Sequential()
lstm_model.add(embedding_layer)
lstm_model.add(Dropout(0.2))
lstm_model.add(Conv1D(filters=32, kernel_size=5, padding='same', activation='relu'))
lstm_model.add(MaxPooling1D(pool_size=2))
lstm_model.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))
lstm_model.add(MaxPooling1D(pool_size=2))
lstm_model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
lstm_model.add(BatchNormalization())
lstm_model.add(Dense(256, activation='relu'))
lstm_model.add(Dense(128, activation='relu'))
lstm_model.add(Dense(64, activation='relu'))
lstm_model.add(Dense(2, activation='softmax'))
lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
print(lstm_model.summary())
lstm_model.fit(x_train, y_train, epochs=1, batch_size=300)

lstm_model.save('lstm.h5')

#Training Gated Recurrent Unit (GRU) Model
embedding_vecor_length = 32
gru_model = Sequential()
gru_model.add(embedding_layer)
gru_model.add(GRU(100, dropout=0.2, recurrent_dropout=0.2))
gru_model.add(BatchNormalization())
gru_model.add(Dense(2, activation='softmax'))
gru_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
print(gru_model.summary())
gru_model.fit(x_train, y_train, epochs=1, batch_size=300)
gru_model.save('gru.h5')

